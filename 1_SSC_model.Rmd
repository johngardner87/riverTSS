---
title: "1_SSC_model"
author: "John Gardner and Simon Topp"
date: "November 18, 2019"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(feather)
library(viridis)
library(knitr)
library(sf)
library(rgdal)
library(maps)
library(magrittr)
library(mlbench)
library(caret)
library(doParallel)
library(xgboost)
library(Metrics)
library(ggplot2)
library(CAST)
library(future)

knitr::opts_chunk$set(echo = TRUE)
```


```{r prepdata}

iter <- "xgb_v1.2"

#Clean up the matchups from Aqusat
cons.out <- read_feather('D:/Dropbox/projects/riverTSS/3_prepdata/out/parametersMunged_corrected_v4.feather') 

outliers<- read_csv("D:/Dropbox/projects/riverTSS/2_model/out/outlier_20230120.csv") 
outliers_new <- outliers %>%
  filter(actual_max < 1000 | (actual_max > 1000 &  count > 200 )) 

# Filter matchups to high quality data and remove outliers
data <- cons.out %>%
  filter(!uniqueID %in% outliers_new$uniqueID) %>%
  mutate(timediff= timediff / (60*60)) %>%
  filter(parameter == "tss") %>%
  filter(!is.na(value)) %>%
 filter(!analytical_method %in% c("UNKOWN", "SSC by filtration (D3977;WI WSC)", "Sediment conc by evaporation", "Historic", "Filterable Residue - TDS", "Cheyenne River Sioux Tribe Quality Assurance Procedures")) %>%
  group_by(analytical_method) %>%
  mutate(n=n()) %>%
  filter(n > 10) %>%
  ungroup() %>%
  filter( dswe <=1)  %>%
  filter(hillshadow ==1 | is.na(hillshadow)) %>%
    filter (type == "Estuary" &  between(timediff, -7, 7) |
          type %in% c("Lake", "Facility") & between(timediff, -20, 20) |
          type == "Stream" & between(timediff, -20, 20) 
            ) %>%
  filter(type == "Lake"  & !is.na(COMID_lake) |
           type %in% c("Stream", "Facility", "Estuary") &     is.na(COMID_lake)) %>%
  filter(value > 0.1) %>%
  filter(value < 5000) %>% #
  filter(!value %in% c(1, 2, 3, 4)) %>%
  filter(type == "Stream" & offset < 0.05 | 
          type == "Lake" & offset <0.2 |
          is.na(offset) )  %>%
  filter(TotDASqKM > 50 | is.na(TotDASqKM)) %>%
  filter(!grepl("USGS", SiteID) & !grepl("SSC", characteristicName)) %>%
  filter(dw < 699) %>%
  filter(dw >= 400) 
  

# SSC is highly skewwed data. To not overfit to the most common values, lets flatten the SSC distribution by removing a percent of the most common SSC values proportional to the spatial data density
prPercent<- data %>%
  mutate(pathrow = paste0(path, row, sep="")) %>%
  group_by(pathrow) %>%
  summarise(pathrowCount = n()) %>%
  ungroup() %>%
  mutate(pathrowP = pathrowCount/sum(pathrowCount)) %>%
  select(pathrow, pathrowP)


throwOut <- data %>%
  mutate(pathrow = paste0(path, row, sep="")) %>%
  select(pathrow, uniqueID, value) %>%
  left_join(prPercent, by="pathrow") %>%
  filter(value >= quantile(data$value, probs = .25) & 
           value <= quantile(data$value, probs = .75) & 
           pathrowP > quantile(prPercent$pathrowP, prob=.9)) %>%
  select(-pathrowP) %>%
  group_by(pathrow) %>%
  nest() %>%
  ungroup() %>%
  left_join(prPercent, by="pathrow") %>%
  mutate(samp = map2(data, pathrowP * (0.9/quantile(prPercent$pathrowP, prob=1)), sample_frac)) %>% 
  select(-data) %>%
  unnest(samp)

data <- data %>%
  filter(!uniqueID %in% throwOut$uniqueID)


# function split data representatively across time, space, concentration for training and hold-out testing
holdout <- function(x) {

  x <- x %>%
  group_by(long_group, time_group) %>%
  dplyr::mutate(mag = cut(value, quantile(
  x = value,
  c(0, 0.2, 0.4, 0.6, 0.8, 0.85, 0.9, 0.95,  1),
  include.lowest = T
  )),
  mag = factor(
  mag,
  labels = c(0.2, 0.4, 0.6, 0.8, 0.85, 0.9, 0.95, 1)
  )) %>%
  ungroup()
  
  set.seed(22)
  
  t <- x %>%
  group_by(time_group, long_group, mag) %>%
  sample_frac(.9) %>%
  ungroup() %>%
  dplyr::mutate(.partitions = 1)
  
  v <- x %>%
   anti_join(t) %>%
   dplyr::mutate(.partitions = 2)

  out <- t %>%
  bind_rows(v) 
    
  return(out)
}

# make training and validation data
df <- data %>%
  mutate(lat_group = cut_number(lat, 2, right= F),
         long_group = cut_number(long, 3, right=F),
         date = lubridate::ymd(date),
         julian = as.numeric(julian.Date(date)),
         space_group = paste0(lat_group,long_group),
         time_group = cut_number(julian, 3, right=F)) %>%
         holdout() %>% 
         ungroup() %>%
         mutate(value = log(value)) %>%
         as.data.frame()

train <- df %>%
  filter(.partitions ==1) %>% 
  ungroup() %>%
  as.data.frame() %>%
  filter(across( c(features_1), ~ !is.nan(.))) %>%
  filter(across( c(features_1), ~ !is.infinite(.))) %>%
  filter(across( c(features_1), ~ !is.na(.)) )

validate <- df %>%
  filter(.partitions ==2) %>%
  ungroup() %>%
  as.data.frame()

val.cols <- df %>% 
  filter(.partitions ==2) %>%
  ungroup() 

```



```{r initialtest}

### Do a quick test. Make a model without optimizing

#load features that were important in past models or manually chose columns/features
ffsResults <- read_feather(paste0('D:/Dropbox/projects/riverTSS/2_model/out/ffsResults_', iter, '.feather'))
                           
features_1 <- ffsResults[ffsResults$RMSE == min(ffsResults$RMSE),] %>%
  select(-c(nvar, RMSE, SE)) %>%
  paste(.) %>% .[.!= 'NA'] %>%
  as.vector()

# make default parameters
grid <- expand.grid(
  nrounds = 100,
  alpha = 0,
  lambda = 1,
  eta = 0.3
)

# identify folds and variables for spatial-temporal cross validation
folds <- CreateSpacetimeFolds(train,
  spacevar = "long_group",
  timevar = "time_group" ,
  k = 3)

# set control variables for xgboost model  
control <- trainControl(
  method = "cv",
  savePredictions = 'none',
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  p = 0.8)

# train model
model <- caret::train(
  x = train[, features_1],
  y = train$value,
  trControl = control,
  tuneGrid = grid,
  method = "xgbLinear",
  importance = T,
  verbose = TRUE
)


```



```{r map}
# make map of matchup locations
data_map <- data %>% 
  group_by(SiteID,lat, long) %>%
  summarise(Count = n()) %>%
  ungroup() %>%
  st_as_sf(coords = c("long", "lat"), crs=4326) %>%
  st_transform(2163) %>%
  mutate(Count = log10(Count))

usa <- st_as_sf(map('usa', plot = FALSE, fill = TRUE)) %>%
  st_combine()  %>%
  st_transform(2163)
  
ggplot() +
  geom_sf(data=usa, color="black", fill="white") +
  geom_sf(data=data_map, aes(color=Count), alpha=0.5, size=0.8) +
  scale_color_viridis() +
  scale_color_viridis_c(breaks = c(0,1,100), labels=c(1, 10, 100)) +
  theme(legend.position = c(0.98, 0.29),
    line = element_blank(), 
    rect = element_blank(), 
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.background = element_blank(),
    legend.text = element_text(size=9))

#ggsave("D:/Dropbox/projects/riverTSS/figs/sampling_density.png", width=6, height=3, units="in", dpi=300)
```


```{r ffs, echo=F, message=F}

# select potential features to use in feature selection
features <- data %>%
  dplyr::select( blue, green, red, nir, swir1, swir2, NR:dw) %>%
  dplyr::select(-NS_NR) %>%
  names(.)

#
set.seed(10)

folds <- CreateSpacetimeFolds(train,
  spacevar = "long_group",
  timevar = "time_group" ,
  k = 3)
  
control <- trainControl(
  method = "cv",
  savePredictions = 'none',
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  p = 0.8)
  
# Do initial feature selection with default hyperparameters
tuneGrid1 <- expand.grid(
  nrounds = 100,
  alpha = 0,
  lambda = 1,
  eta = 0.3
)


# Set it up to run in parallel. This can take 1-2 days depending on number of features and rows
cl <- makePSOCKcluster(availableCores() - 2)
registerDoParallel(cl)

ffs <- ffs(train[,features], train$value, method = 'xgbLinear', metric = 'RMSE', tuneGrid = tuneGrid1, Control = control, verbose = T)

on.exit(stopCluster(cl))
registerDoSEQ()

ffsResults <- ffs$perf_all

# Save the results
#write_feather(ffsResults, paste0('D:/Dropbox/projects/riverTSS/','2_model/out/ffsResults_',iter,'.feather'))
#save(ffs, file=paste0('D:/Dropbox/projects/riverTSS/2_model/out/ffs_',iter,'.RData'))

ffsResults %>%
  group_by(nvar) %>%
  summarise(RMSE = median(RMSE),
            SE = median(SE)) %>%
  ggplot(.) + geom_line(aes(x = nvar, y = RMSE)) +
  geom_errorbar(aes(x = nvar, ymin = RMSE - SE, ymax = RMSE + SE), color = 'red')

#ggsave(paste0('figs/rfeRMSE_', iter, '.png'), device = 'png', width = 6, height = 4, units = 'in')

```


```{r final_model}

# select features with the lowest error and lowest number of features
features_1 <- ffsResults[ffsResults$RMSE == min(ffsResults$RMSE),] %>%
  dplyr::select(-c(nvar, RMSE, SE)) %>%
  mutate(cna = rowSums(is.na(.)) ) %>%
  filter(cna == max(cna)) %>%
  select(-cna) %>%
  paste(.) %>% .[.!= 'NA'] %>%
  as.vector()

# make a grid of hyperparameters for tuning
grid_base <- expand.grid(
  nrounds = c(50,100,200,300),
  alpha = c(0, 0.001, 0.01, 1),
  lambda = c(0.01, 0.1, 1, 5, 10),
  eta = c(0.01, 0.1, 0.3, 0.6 )
)

set.seed(10)


folds <- CreateSpacetimeFolds(train, spacevar = "long_group", timevar = "time_group" , k=3)

#Set up a cluster to run everything in parallel
cl <- makePSOCKcluster(availableCores()-2)
registerDoParallel(cl)

train_control <- caret::trainControl(
  method = "cv",
  savePredictions = F,
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  verboseIter = T,
  allowParallel = TRUE,
  p = 0.8
  )
  
base_model <- caret::train(
  x = train[,features_1],
  y = train$value,
  trControl = train_control,
  tuneGrid = grid_base,
  method = "xgbLinear",
  verbose = TRUE,
  importance = F
)

base_model$bestTune

train_control_final <- caret::trainControl(
  method = "cv",
  savePredictions = T,
  returnResamp = 'final',
  index = folds$index,
  indexOut = folds$indexOut,
  verboseIter = T,
  allowParallel = T,
  p = 0.8
  )
  
grid_final <- expand.grid(
  nrounds = base_model$bestTune$nrounds,
  alpha = base_model$bestTune$alpha,
  lambda =  1, # base_model$bestTune$lambda
  eta = base_model$bestTune$eta
)

model <- caret::train(
  x = train[, features_1],
  y = train$value,
  trControl = train_control_final,
  tuneGrid = grid_final,
  method = "xgbLinear",
  importance = T,
  verbose = TRUE
)

stopCluster(cl)


# helper function for plot
tuneplot <- function(x, probs = .90) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
    theme_bw()
}

tuneplot(base_model)

#ggsave(paste0("D:/Dropbox/projects/riverTSS/figs/base_tune_",iter, ".tiff"), width=10, height = 8, units="in", dpi=300)

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

```


```{r save}

# apply model to hold-out testing data
pred<- predict( model, validate[,features_1])
actual <- (validate$value)
uniqueID <- val.cols$uniqueID

output <- tibble(Predicted = pred, Actual = actual,uniqueID = uniqueID) %>%
  mutate(Actual = exp(Actual), Predicted = exp(Predicted)) %>%
  left_join(cons.out %>%
              dplyr::select(-c(TOT_BASIN_AREA:TOT_WB5100_ANN)), by="uniqueID") %>%
  mutate(residual = Actual - Predicted,
         obs = ifelse(abs(residual) > quantile(abs(residual), .975, na.rm=T), "bad", "good"))


### SAVE
#write_csv(data %>%
#            select(-TOT_BASIN_AREA:ACC_EVI_AMJ_2012), path = #'D:/Dropbox/projects/riverTSS/2_model/out/Aquasat_TSS_v1.1.csv')

#write_feather(train, path = paste0('D:/Dropbox/projects/riverTSS/4_model/out/train_',iter, '.feather'))
#write_csv(train, path = paste0('D:/Dropbox/projects/riverTSS/2_model/out/train_',iter, '.csv'))

#write_feather(train %>% select(value, features_1), path = paste0('D:/Dropbox/projects/riverTSS/4_model/out/train_clean_',iter, '.feather'))

#write_csv(train %>% select(value, features_1), path = paste0('D:/Dropbox/projects/riverTSS/2_model/out/train_clean_',iter, '.csv'))

#write_feather(df, path ='4_model/out/Aqusat_TSS_v1.feather')
#write_csv(df, path = paste0('D:/Dropbox/projects/riverTSS/2_model/out/matchups_TSS_', iter, '.csv'))


#write_feather(output, path = paste0('D:/Dropbox/projects/riverTSS/4_model/out/output_',iter, '.feather')
#write_csv(output, path = paste0('D:/Dropbox/projects/riverTSS/2_model/out/output_',iter, '.csv'))

### save the model
#save(model, file = paste0('D:/Dropbox/projects/riverTSS/2_model/out/finalmodel_',iter, '.Rdata'))
#saveRDS(model, file = paste0('D:/Dropbox/projects/riverTSS/2_model/out/finalmodel_',iter, '.rds'))

```


```{r evaluate}

# function for relative bias or error
Rerror <- function(y, y_hat) {
   # called "% Bias" in from Dethier et al. 2020
   x<- (10^median(abs(log10(10^y_hat/10^y)), na.rm = T)-1)

return(x)
}

# calcualte error metrics   
evals <- output %>%
  mutate(Actual = (Actual), 
         Predicted = (Predicted)) %>%
  summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            mape = mape(Actual, Predicted),
            bias = bias(Actual, Predicted),
            p.bias = percent_bias(Actual, Predicted),
            smape = smape(Actual, Predicted),
            Rerr = Rerror(log10(Actual), log10(Predicted))) 


ggplot(output, aes(x = (Actual), y = (Predicted))) + 
  geom_point() +
  geom_abline(slope=1, intercept = 0, color = 'black')+
  xlab("Measured SSC (mg/L)") +
  ylab("Predicted SSC (mg/L)")+
  theme_bw() +
  scale_x_log10() +
  scale_y_log10() +
  theme(text = element_text(size=18),
        legend.position = c(0.25,0.75),
        legend.background = element_blank()) 


#ggsave('D:/Dropbox/projects/riverTSS/figs/ModelValidation_20230206.png', width = 5, height = 5, units = 'in' ,dpi=350)

ggplot(output, aes(x = year, y = residual)) + 
  geom_point() +
  xlab("year") +
  ylab("Residual")+
  theme_few() +
  theme(text = element_text(size=18))

# check errors metrics across space, time, and concentraiton 
errorSum <- output %>%
  mutate(Observed.Value = Actual) %>%
  mutate(Year = lubridate::year(date)) %>%
  rename( Latitude = lat, Longitude = long) %>%
  mutate(Longitude = round(Longitude, 1)) %>%
  gather(Observed.Value, Year, Latitude, Longitude, key = 'Variable', value = 'Value') %>%
  group_by(Variable) %>%
  mutate(quantile = ifelse(Variable == "Observed.Value", 
                   cut_number(Value, n=5, right = F, breaks= c(0, 10, 100,1000, 10000, 20000), labels = F),
                   cut_number(Value, n=4, right = F, labels = F))) %>%
    mutate(quantLabs = ifelse(Variable == "Observed.Value", as.character(case_when(
      quantile == 1 ~ "0-10",
      quantile == 2 ~ "10-100",
      quantile == 3 ~ "100-1000",
      quantile == 4  ~ "1k-10k",
      quantile == 5  ~ "outlier"
    )),
    as.character(cut_number(Value, 4,  right = F, dig.lab = 3)))) %>% 
 ungroup() %>%
  group_by(quantile, quantLabs, Variable) %>%
  dplyr::summarise(rmse = rmse(Actual, Predicted),
            mae = mae(Actual, Predicted),
            bias = bias(Actual, Predicted),
            pbias = percent_bias(Actual, Predicted),
            rbias = Rerror(log10(Actual), log10(Predicted))) %>%
  gather(rmse:rbias, key = 'Metric', value = 'Error') %>%
  as.data.frame() %>%
  arrange(Variable, quantile) %>%
  mutate(order = row_number())

ggplot(errorSum %>% 
         filter(quantLabs != "outlier") %>%
         filter(Metric == 'pbias'|Metric=="mae") %>%
         mutate(Error = ifelse(Metric == "pbias", Error*100, Error)), 
       aes(x = fct_reorder(quantLabs,order), y = Error, color = Metric, group = Metric)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~Variable, scales = 'free')  +  
  theme_bw() +
  scale_color_viridis(discrete = T) +
  #scale_y_continuous(labels = scales::percent) +
  theme(axis.text.x = element_text(angle = 90, vjust = .5),
        legend.position = 'bottom') +
  labs(x = 'Quantile', y = 'Error')

#ggsave("D:/Dropbox/projects/riverTSS/figs/error_groups202303.png", width=6, height = 6, dpi = 300, units= "in")

# make variable importance plot
model$modelInfo$varImp(model$finalModel) %>%
  mutate(Feature = fct_reorder(rownames(.), Overall, .desc = F)) %>%
  arrange(Overall) %>%
  filter(Feature!="GR") %>%
  ggplot(., aes(x = Feature, y = Overall)) + 
  geom_col() +
  scale_x_discrete(labels = c( 'Swir2', "Nir-Red", "Blue/(Red+Green)", "Green/(Red+Nir)", "NDVI", 'Swir1', "Green", "Hue",  'Nir',  "Red/(Blue+Swir1)" ) ) +  
  coord_flip() +
  theme_bw() +
  labs(y = 'Importance (Model Gain)') 

#ggsave("D:/Dropbox/projects/riverTSS/figs/feat_importance_202303.png", width=4, height = 3, dpi = 300, units= "in")

```


```{r predict, echo=F, message=F}

# load data SR data
sr_clean <- read_feather("D:/Dropbox/projects/rivercolor/out/riverSR_usa_v1.1.feather")
 
# load features
ffsResults <- read_feather(paste0('D:/Dropbox/projects/riverTSS/2_model/out/ffsResults_', iter, '.feather'))
# load model
model <- readRDS(paste0('D:/Dropbox/projects/riverTSS/2_model/out/finalmodel_',iter, '.rds'))

features_1 <- ffsResults[ffsResults$RMSE == min(ffsResults$RMSE),] %>%
  dplyr::select(-c(nvar, RMSE, SE)) %>%
  mutate(cna = rowSums(is.na(.)) ) %>%
  filter(cna == max(cna)) %>%
  select(-cna) %>%
 # drop_na() %>%
  paste(.) %>% .[.!= 'NA'] %>%
  as.vector()

# apply model
pred <- tibble(tss = exp(predict(model, sr_clean[,features_1])))

sr_tss <- sr_clean %>%
  bind_cols(pred)

#Save final database
sr_tss <- sr_tss %>%
  distinct(ID, date, path, row, sat, blue, .keep_all = TRUE) %>%
  select(LS_ID, ID, date, time, year, month, season, decade,  tss, Cloud_Cover, azimuth, zenith, elevation, hillshadow, dswe, path, row, sat, pixelCount, count,  flag, qa, blue_raw:swir2, mean_SR, rn)

#write_csv(sr_tss, "D:/Dropbox/projects/riverTSS/out/RiverSed_USA_V1.1.txt")

#write_feather(sr_tss, "D:/Dropbox/projects/riverTSS/out/RiverSed_USA_V1.1.feather")

```


