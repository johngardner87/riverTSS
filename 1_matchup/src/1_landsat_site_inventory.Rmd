---
title: "landsat_site_inventory"
author: "John Gardner"
date: "July 15, 2019"
output: html_document
---


# Subsetting Water Quality Portal sites to landsat visible pixels.

The water quality portal gives us a big inventory of sites with siteids and lat longs. Lots of these sites are on small rivers or have multiple characteristics associated with them. So we need to create a subset of the data that is only landsat visible. For now this involves some initial cleaning of the data in R. Sending the data to earth engine and then checking if the water pixels associated with the point are landsat visible (generally wider than 60meters)

Obviously this is not in the remake style yet. Future goals. 


```{r setup}
library(knitr)
library(feather)
library(googledrive)
library(tidyverse)
library(LAGOSNE)
library(reticulate)
library(sf)
#opts_knit$set(root.dir='../..')
```

## Grab all unique water quality sites. 

Uniqueness is defined by a unique combination of site name and lat and long.

```{r}
#Read in inventory and exclude nutrient data (not a goal of this project just yet)
inv_simple <- #read_feather('1_wqdata/out/wqp_inventory.feather') %>%
  read_feather("data/wqp_inventory.feather") %>%
  select(SiteID=MonitoringLocationIdentifier,lat=LatitudeMeasure,
         long=LongitudeMeasure,orig_datum = HorizontalCoordinateReferenceSystemDatumName,
         Constituent,obs_count=resultCount) %>% 
  filter(!is.na(lat)) %>% #Exclude sites with missing geospatial data
  filter(!is.na(long)) %>%
  mutate(source='WQP')

#write_feather(inv_simple,'1_wqdata/out/inv_simple.feather')
#Make a table of all the projections
table(inv_simple$orig_datum)
  
#Setup a datum table to transform different datums into WGS84 for GEE and to 
#match LAGOS. Note that we make the somewhat reisky decision that others and unknowns
# are mostly NAD83 (since > 60% of the data is originally in that projection)
datum_epsg <- tibble(orig_datum=c('NAD27','NAD83','OTHER','Unknown','UNKWN','WGS84','WGS72'),
                     epsg=c(4267,4269,4269,4269,4269,4326,4322))
# Get distinct lat longs and sites
inv_uniques <- inv_simple %>%
  distinct(SiteID,lat,long,orig_datum)

## project inventory using the above lookup table
## Have to loop over each projection
projected <- list() 
for(i in 1:nrow(datum_epsg)){
  #Select each datum iteratively
  d1 <- datum_epsg[i,]
  # Join inventory to that datum only and then feed it that CRS and then 
  # transform to wgs84
  inv_reproj <- inv_uniques %>%
    filter(orig_datum == d1$orig_datum) %>%
    inner_join(d1, by='orig_datum') %>%
    st_as_sf(.,coords=c('long','lat'),crs=d1$epsg) %>%
    st_transform(4326)
  
  projected[[i]] <- inv_reproj
}
# Note that we lose some sites in GUAM which is intentional
# Add back in lat longs which are all WGS84 now
inv_wgs84 <- do.call('rbind',projected) %>%
  mutate(lat = st_coordinates(.)[,2],
         long=st_coordinates(.)[,1]) %>%
  as.data.frame(.) %>%
  select(-geometry,-orig_datum) %>%
  mutate(source='WQP')

```

## Merge WQP and LAGOS site info

```{r merge}
#Load in lagos data
lagos.sites <- lagosne_load("1.087.1")$locus %>%
  select(SiteID = lagoslakeid,lat=nhd_lat,long=nhd_long) %>%
  mutate(SiteID = paste('lagos',SiteID,sep='-')) %>%
  mutate(source='LAGOS') %>%
  mutate(epsg=4326) %>%
  select(names(inv_wgs84))

#load in new usgs SSC data
library(cdlTools)
usgs_site <- read_csv("data/discrete_sites.csv") %>%
  mutate(SITE_NO = as.character(SITE_NO))

## load unique SSC sites combined usgs and wqp
usgs.sites<- read_csv("data/USGS_tis.csv") %>%
  mutate(SITE_NO = as.character(SITE_NO),
         dt = as.POSIXct(DATETIME, format="%m/%d/%Y %H:%M", tz="GMT") ,
         date =as.Date(dt) ) %>%
  left_join(usgs_site, by="SITE_NO") %>%
  filter(SSC > 0, SSC < 10^5, !is.na(SSC)) %>%
  mutate(SiteID = paste("USGS-",SITE_NO, sep="")) %>%
  rename(tss = SSC, date_unity = dt,  MonitoringLocationName=STATION_NM,HUCEightDigitCode = HUC_8, lat=LATITUDE, long = LONGITUDE, StateCode = STATE,
         CountyCode = COUNTY_NAME, resultCount=sample_count) %>%
  select(SiteID, date, tss, lat, long, date_unity, resultCount, 
         MonitoringLocationName, HUCEightDigitCode, StateCode, CountyCode ) %>%
  mutate(MonitoringLocationTypeName = "Stream",
         ResolvedMonitoringLocationTypeName = "Stream",
         CountryCode="US",
         source="USGSsed",
         Constituent = "tis",
         HorizontalCoordinateReferenceSystemDatumName="NAD83",
         HUCEightDigitCode =  as.character(HUCEightDigitCode),
         StateCode = fips(StateCode, to="FIPS"),
         StateCode = str_pad(StateCode, 2, pad="0"),
         date_only=FALSE,
         epsg = 4269) %>%
  select(SiteID, epsg, lat, long, source)


#Bind lagos and wqp data together
lagos.wqp <- bind_rows(lagos.sites,inv_wgs84, usgs.sites) %>%
  distinct(SiteID, lat, long, source) %>%
  arrange(lat,long)

#Save this data and push to drive
data_file <-'output/unique_site_inventory_v2.feather'
write_feather(lagos.wqp, data_file)
#scipiper::gd_put(scipiper::as_ind_file(data_file), data_file)
```


## Check if sites are visible as a water body in Landsat. 

Most of the data living in the Water Quality Portal is on streams, lakes, and/or estuaries that are too small to be visible as water bodies from Landsat satellites, which have a resolution of ~30m. In this project we only want to keep sites that are consistently close to areas classified as water. Luckily there is a really nice paper and dataset from Jean Francois Pekel and others, where they looked at the entire history of Landsat 5-8 and classified how often any given pixel was classified as water. Data from [this paper](https://www.nature.com/articles/nature20584) is available in GEE and we use it to only keep sites that are close to water bodies visible from landsat. 

The code below checks all sites for water pixels within 200m of the lat long of sites in the WQP. It takes 20 minutes or so to run with ~ 400000 sites

```{r reticulate_version}
#reticulate_python <- yaml::yaml.load_file('lib/cfg/gee_config.yml')$RETICULATE_PYTHON
#use_python(reticulate_python)

reticulate_python <-'C:/Users/john/Anaconda2/envs/earthEngineGrabR/python.exe'
Sys.setenv(RETICULATE_PYTHON = reticulate_python)
use_condaenv("earthEngineGrabR", required=TRUE)



```


## Does the earth engine script need to be reran?
```{r}
#Point to the temp folder where site matchup data goes
folder <- drive_ls('tempSiteWater')
#Check how many 5000 file chunks there will be. 
should_be <-
  #read_feather('2_rsdata/out/unique_site_inventory.feather') %>% 
  read_feather('output/unique_site_inventory_v2.feather') %>% 
  nrow(.)/5000  
should_be <- ceiling(should_be)
do_it = !nrow(folder) == should_be
if(nrow(folder) < should_be & nrow(folder) != 0){
  for(i in 1:nrow(folder)) {
  drive_rm(as_id(folder$id[i]))
  }
}
```

```{python,eval=do_it}
#Import Libraries
import ee
ee.Initialize()
import pandas as p
import feather as f
import time
#Read in the water mask
pekel = ee.Image('JRC/GSW1_0/GlobalSurfaceWater')
#Point to the inventory feather

#file = '2_rsdata/out/unique_site_inventory.feather'
file = 'output/unique_site_inventory_v2.feather'

#Read in our inventory feather
inv = f.read_dataframe(file)
#Get the length of unique sites
obs = len(inv.index) 
#To convert these sites into featurecollections we have to split it up 
#in a loop. 
#List comprehension loop requires exact indecis to convert site lat long
#into gee collections
splitStart = list(range(0, obs, 5000))
splitEnd=list(range(5000,obs + 5000 ,5000))
splitEnd[-1]=obs
#Selct the occurence layer in the pekel mask, which is just the percentage of water occurence
#over a given pixel from 1985-2015. 
occ = pekel.select('occurrence')
#####----------Earth Engine Pull here ---------######
#Define our water extraction function outside of the loop 
#so it is not defined over and over
def waterfunc(buf):
  #Define a 200m buffer around each point
  invBuf = buf.buffer(200).geometry()
  #Clip the pekel mask to this buffer
  pekclip = occ.clip(invBuf)
  #Reduce the buffer to pekel min and max
  pekMin = pekclip.reduceRegion(ee.Reducer.minMax(), invBuf, 30)
  #Add another reducer to get the median pekel occurnce
  pekMed = pekclip.reduceRegion(ee.Reducer.median(),invBuf,30)
  #Define the output features
  out = buf.set({'max':pekMin.get('occurrence_max')})\
          .set({'min':pekMin.get('occurrence_min')})\
          .set({'med':pekMed.get('occurrence')})
          
  return out
#Source function to limit number of tasks sent up to earth engine.
exec(open("D:/Dropbox/projects/TSS/code/GEE_pull_functions_matchups.py").read())    

#execfile('D:/Dropbox/projects/TSS/code/GEE_pull_functions_matchups.py')
      
#Loop over the index stored in split
for x in range(0,len(splitStart)):
#turn our inventory into a feature collection by assigning lat longs and a site id.
#This is done via list comprehension which is similar to a for loop but faster and
#plays nice with earth engine.  Collections are limited for 5000 to avoid time outs
#on the server side.
  invOut = ee.FeatureCollection([ee.Feature(
    ee.Geometry.Point([inv['long'][i], inv['lat'4][i]]),
    {'SiteID':inv['SiteID'][i]}) for i in range(splitStart[x], splitEnd[x])]) 
  #Map this function over the 5000 or so created sites
  outdata = invOut.map(waterfunc)
  #Define a data export 
  dataOut = ee.batch.Export.table.toDrive(collection = outdata,
                                          description = "LandsatSitePull" + str(x),
                                          folder='tempSiteWater',
                                          fileFormat = 'csv')
  #Send next task.
  dataOut.start()
#Make sure all Earth engine tasks are completed prior to moving on.  
maximum_no_of_tasks(1,60)
print('done')
## End the python bash.
```



## Local download

```{r}
#Download data from python output stored in local googledrive folder called tempSiteWater
folder <- drive_ls('tempSiteWater')
#Download files locally.
for(i in 1:nrow(folder)) {
  path=paste0('2_rsdata/tmp/water_visible_sites/',folder$name[i])
  drive_download(as_id(folder$id[i]),
                 path=path)
}
# Move files from personal drive folder into team drive folder. 
for(i in 1:nrow(folder)) {
  drive_mv(as_id(folder$id[i]), path = '2_rsdata/tmp/water_visible_sites/')
}
```

## Read in data and save as a single file

This produces a final water inventory if the max pekel occurence is greater than 80 (later steps can make this more strict).

```{r}
#Get a list of those files
water.inv.files <- #list.files('2_rsdata/tmp/water_visible_sites',full.names = T)
  list.files('D:/GoogleDrive/tempSiteWater', full.names = T)
#Use map_df to read them all in and stitch them together
water.inv <- map_df(water.inv.files,read_csv)

#Keep only sites with max pekel > 80 (this number is arbitrary and can be changed)
#Extract the coordinates from the .geo using str split
pekel.max <- 40
water.80 <- water.inv %>%
  filter(max > pekel.max) %>%
  mutate(coords = str_split_fixed(`.geo`,'\\[',2)[,2] %>%
           gsub(']}','',.),
        long = as.numeric(str_split_fixed(coords,',',2)[,1]),
        lat = as.numeric(str_split_fixed(coords,',',2)[,2])) %>%
  select(-`.geo`,-coords,-`system:index`)

# write the file
data_file <- 'D:/Dropbox/projects/TSS/output/unique_site_visible_inv_v2.feather'
write_feather(water.80,data_file)


#Upload to team drive
#scipiper::gd_put(scipiper::as_ind_file(data_file), data_file)
```