---
title: "0_CloudScenes"
output:
  html_document:
    toc: true
    toc_depth: 3
editor_options: 
  chunk_output_type: console
---


# Gettting Landsat Cloud Scores

To save some computation later, we decided to download each landsat cloud score from Google Earth Engine. That way, when we eventually join the water quality data to reflectance infortmation we don't ask for data over water quality sites that are too small to be observed by landsat or on days when it is too cloudy to get reliable data. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir='../..')
library(tidyverse)
library(reticulate)
library(googledrive)
library(lubridate)
library(feather)
## HTTP 2 causes error force http1.1
#httr::set_config(config(http_version = 1.1)) # set the HTTP version to 1.1 (none, 1.0, 1.1, 2)
```


## Surface Reflectance Cloudiness pull from GEE

The code below grabs cloud scores, WRS PATH and ROW, landsat ID and date information from google earth engine using python code. The data is gathered only for the contiguous USA. 

We use the USGS surface reflectance product for this data pull. Information on this data can be found [here](https://landsat.usgs.gov/landsat-surface-reflectance).

```{r reticulate_version}
reticulate_python <- yaml::yaml.load_file('lib/cfg/gee_config.yml')$RETICULATE_PYTHON
use_python(reticulate_python)
```


## Does the earth engine script need to be reran?
```{r}
folder <- drive_ls('clouds')
should_be = year(Sys.Date())-1983
do_it = !nrow(folder) == should_be



if(nrow(folder) < should_be & nrow(folder) != 0){
  for(i in 1:nrow(folder)) {
  drive_rm(as_id(folder$id[i]))
  }
}


```


```{python,eval=do_it}
#Import libraries
import ee
import time
import numpy
#Initialize earth engine
ee.Initialize()

## Define a function to get properties from the landsat stack
def getProperties(i):
    return (ee.Feature(None, i.toDictionary(
        ['CLOUD_COVER', 'WRS_PATH', 'WRS_ROW',
        'LANDSAT_ID', 'DATE_ACQUIRED',
        'SENSING_TIME', 'SOLAR_AZIMUTH_ANGLE', 
        'SOLAR_ZENITH_ANGLE'])))
        

#Pull out USA geometry from earth engine boudary layer
usa = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017').filter(ee.Filter.eq('country_co','US')).geometry()

#Import landsat surface reflectance collections.
l8 = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')
l7 = ee.ImageCollection('LANDSAT/LE07/C01/T1_SR') 
l5 = ee.ImageCollection('LANDSAT/LT05/C01/T1_SR')

#Merge landsat collections
lsCollection= ee.ImageCollection(l5.merge(l7).merge(l8)).filterBounds(usa)

#Define a month day to start years (Jan 1)
md = '-01-01'

#Get data for years of Landsat 5-8 operation. 
years=numpy.arange(1984,2020,1)




#Source function to limit number of tasks sent up to earth engine.
exec(open("2_rsdata/src/GEE_pull_functions.py").read())    


#Loop over years such that we download a single csv for each year. 
for year in years:
  print(year)
  #Filter landsat stack to the year in the loop (starting with 1984)
  lsFiltered = lsCollection.filterDate(str(year) + md, str(year + 1) + md)
  #Get the properties from all the scenes for that year. 
  Cloudiness = lsFiltered.map(getProperties)
  #Define an export task to export the data
  task0 = ee.batch.Export.table.toDrive(
       collection = Cloudiness,
       description = 'Cloudiness' + str(year),
       folder = 'clouds',
       fileNamePrefix = str(year) ,
       fileFormat = 'CSV')
  
  #Check how many existing tasks are running and take a break if it's >15
  maximum_no_of_tasks(15, 30)
  #Send next task.
  task0.start()

#Make sure all Earth engine tasks are completed prior to moving on.  
maximum_no_of_tasks(1,60)
print('done')

## End the python bash.
```


## Download GEE data from google drive

Now that GEE has created and exported a summary of cloud scores for all landsat scenes over the USA, we want to download that data to our local computer. 

```{r}
#List files
folder <- drive_ls('clouds')
folder
#Download files locally.
for(i in 1:nrow(folder)) {
  path=paste0('2_rsdata/tmp/yearly_clouds/',folder$name[i])
  drive_download(as_id(folder$id[i]),
                 path=path,
                 overwrite=T)
}

# Move files from personal drive folder into team drive folder. 
for(i in 1:nrow(folder)) {
  drive_mv(as_id(folder$id[i]), path = 'watersat/2_rsdata/tmp/yearly_clouds/')
}


```


## Stitch cloud data together and do some simple scripting to get date 


```{r}

#Read in data. 
clouds <- map_df(list.files('2_rsdata/tmp/yearly_clouds',
                            full.names = T),read_csv) %>%
  mutate(Date = ymd(str_split_fixed(`system:index`,'_',5)[,5])) 

data_file <- '2_rsdata/out/clouds.feather'
#Write out cloud data as a feather
write_feather(clouds, data_file)

#Upload data to team drive
scipiper::gd_put(scipiper::as_ind_file(data_file), data_file)

```

